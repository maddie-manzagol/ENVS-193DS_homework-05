---
title: "code for HW 5"
format: 
  html:
    toc: true
    toc-location: left
    code-fold: true
    theme: yeti 
execute:
  message: false
  warning: false
---
Read in packages
```{r libraries}
# should haves (from last week)
library(tidyverse)
library(here)
library(janitor)
library(ggeffects)
library(performance)
library(naniar) # or equivalent
library(flextable) # or equivalent
library(car)
library(broom)
# would be nice to have
library(corrplot)
library(AICcmodavg)
library(GGally)
```

Read in the csv file
```{r read-in-data}
plant <- read_csv(here("data","knb-lter-hfr.109.18 (1)", "hf109-01-sarracenia.csv")) %>% 
  #make the column names clear
  clean_names() %>% 
  #selecting the columns of interest
  select(totmass, species, feedlevel, sla, chlorophyll, amass, num_lvs, num_phylls)
```

Visulatize the missing data: 
```{r missing-data-visualization}
gg_miss_var(plant)
```

Subsetting the data by dropping NAs:
```{r subset-drop-NA}
plant_subset <- plant %>% 
  drop_na(sla, chlorophyll, amass, num_lvs, num_phylls)
```

Create a correlation plot: 

(example writting) To determine the relationship between numerical variables in our dataset, we calculcated pearson's r and visually represented the correlation using a correlation plot
```{r correlation-plot}
#calculate pearson's r only for numerical values only
plant_cor <- plant_subset %>% 
  select(feedlevel:num_phylls) %>% 
  cor(method = "pearson")

#creating a correlation plot
corrplot(plant_cor,
         #change the shape of what's in the cells
         #point to the left=negative relationship, right=positive relationship
         method = "ellipse",
         #adds the correlation values in black over the elipses
         addCoef.col= "black")
```

Create a plot of each variable compared against the others 
```{r pair-plot}
plant_subset %>% 
  select(species:num_phylls) %>% 
  ggpairs()
```

Starting Regression here: 

(example) To determine how species and physiological characteristics perditc biomass, we fit multiple linear models 
```{r null-and-full-models}
null <- lm(totmass ~ 1, data= plant_subset)
full <- lm(totmass ~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data= plant_subset)
```

We visually assesed normaility and homoskedascitity using diagnostic plots for the full model: (normal, but not homoskedastic because its not entirely random)
```{r full-diagnoistics}
par(mfrow = c(2,2))
plot(full)
```

We also tested for normality using the Shaprio-Wilk test (null hypothesis= variable of interest (ie the residuals) are normally distributed) and homoskedasticity using the Breusch-Pagan test (null hypothesis= variable of interest (ie the residuals) has constant variance)
```{r}
check_normality(full)
check_heteroscedasticity(full)
```

Log transformation: manipulates data to achieve normality 
```{r log transformation}
null_log <- lm(log(totmass)~1, data = plant_subset)
full_log <- lm(log(totmass)~ species + feedlevel + sla + chlorophyll + amass + num_lvs + num_phylls, data= plant_subset)

plot(full_log)
check_normality(full_log)
check_heteroscedasticity(full_log)
```

evaluate multicollinearity: 
We evaluated muticollinarity by calculating the gerneralized variance inflation factor and determined that this model didn't display any multicolinearity because no values were larger than 5 
```{r calculate-vif}
#from the car package
car::vif(full_log)
```
 
try some more models : addressing the question: what set of predictor variables best explains the response? 
```{r}
model2_log <- lm(log(totmass)~ species, data= plant_subset)
```

check assumptions for model 2:
```{r assumptions}
plot(model2_log)

check_normality(model2_log)

check_heteroscedasticity(model2_log)
```

compare models using Akaike's Information criterion (AIC) values: 
```{r AIC-value}
AICc(full_log)
AICc(model2_log)
AICc(null_log)

MuMIn::AICc(full_log, model2_log, null_log)
MuMIn::model.sel(full_log, model2_log, null_log)
```

we compared models using AIC and chose the model with the lowest value, which was full_log model 

#Results 

We found that the ____ model including ____, _____, _____ predictors best predicted ______ (model summary) 
```{r}
summary(full_log)
```

use ggpredict() to back transform estimates 
```{r}
model_pred <- ggpredict(full_log, terms = "species", back.transform = TRUE)

plot(model_pred, add.data = TRUE)

plot(ggpredict(full_log, terms = "chlorophyll", back.transform = TRUE), add.data = TRUE)

plot(ggpredict(full_log, terms = "feedlevel", back.transform = TRUE), add.data = TRUE)
#we didn't like this one^

plot(ggpredict(full_log, terms = "sla", back.transform = TRUE), add.data = TRUE)

model_pred
```

you chose the plot that best displays the data in your own opinion---your report so you chose 
```{r}
#this was already done above
summary(full_log)

table <- tidy(full_log, conf.int = TRUE) %>% 
  #change the pvalue numbers if they are really small
  #chnage the estimates, standard error, and t-statistics to round to ___ digits
  #using mutate 
  #make it into a flex table 
  flextable() %>% 
  # fit it to the viewer 
  autofit()
  

table
```

#different types of ANOVA (tables)


